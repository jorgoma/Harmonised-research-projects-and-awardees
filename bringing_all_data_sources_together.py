# -*- coding: utf-8 -*-
"""Bringing all data sources together.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YnKZEX_htSnyUN5c3Mc09RpNrggFV2hK
"""

import pandas as pd

"""# Loading EPMC"""

epmc_projects = pd.read_csv('/content/drive/MyDrive/Bulk grant data/EPMC/EPMC_projects.csv')
epmc_projects.loc[:, 'award_holder_name'] = epmc_projects['given_name'] + " " + epmc_projects['family_name']
epmc_projects['institution_ror_id'] = 'https://ror.org/' + epmc_projects['institution_ror_id'].astype(str)
epmc_awardees = epmc_projects[['grant_id', 'award_holder_name', 'orcid', 'institution_name']]
epmc_awardees['orcid'] = 'orcid.org/' + epmc_awardees['orcid']

# Count of values including empty/NaN values
currency_counts = epmc_projects['grant_currency'].value_counts(dropna=False)
print("Value counts including empty values:")
print(currency_counts)

# Percentage distribution including empty values
currency_percentage = epmc_projects['grant_currency'].value_counts(dropna=False, normalize=True) * 100
print("\nPercentage distribution including empty values:")
print(currency_percentage)

missing_currency_df = epmc_projects[epmc_projects['grant_currency'].isna()]

# Get distribution of funder_name for these rows
funder_distribution = missing_currency_df['funder_name'].value_counts()
print("Funder distribution for rows with missing grant_currency:")
print(funder_distribution)

# Calculate percentage
funder_percentage = missing_currency_df['funder_name'].value_counts(normalize=True) * 100
print("\nPercentage distribution:")
print(funder_percentage)

import numpy as np
epmc_projects['country'] = 'Unknown'  # Default value

# Apply the conditions
conditions = [
    (epmc_projects['grant_currency'].isna()),  # Missing values
    (epmc_projects['grant_currency'] == 'GBP'),
    (epmc_projects['grant_currency'] == 'EUR'),
    (epmc_projects['grant_currency'] == 'USD'),
    (epmc_projects['grant_currency'] == 'CHF'),
    (epmc_projects['grant_currency'] == 'INR'),
    (epmc_projects['grant_currency'] == 'CAD'),
    (epmc_projects['grant_currency'] == 'BRL'),
    (epmc_projects['grant_currency'] == 'ARS'),
    (epmc_projects['grant_currency'] == 'AUD'),
    (epmc_projects['grant_currency'] == 'JPY'),
    (epmc_projects['grant_currency'] == 'SEK'),
    (epmc_projects['grant_currency'] == 'DKK'),
    (epmc_projects['grant_currency'] == 'NZD'),
    (epmc_projects['grant_currency'] == 'NOK'),
    (epmc_projects['grant_currency'] == 'PLN'),
    (epmc_projects['grant_currency'] == 'PEN'),
    (epmc_projects['grant_currency'] == 'COP'),
    (epmc_projects['grant_currency'] == 'PYG'),
    (epmc_projects['grant_currency'] == 'PAB'),
    (epmc_projects['grant_currency'] == 'HUF'),
    (epmc_projects['grant_currency'] == 'RWF'),
    (epmc_projects['grant_currency'] == 'BGN'),
    (epmc_projects['grant_currency'] == 'ZAR'),
    (epmc_projects['grant_currency'] == 'SGD'),
    (epmc_projects['grant_currency'] == 'MUR'),
    (epmc_projects['grant_currency'] == 'CLP')
]

choices = [
    'Unknown',           # For missing values
    'GB',           # For GBP
    'Based in EU',  # For EUR
    'US',           # For USD
    'Switzerland',  # For CHF
    'India',        # For INR
    'Canada',       # For CAD
    'Brazil',       # For BRL
    'Argentina',    # For ARS
    'Australia',    # For AUD
    'Japan',        # For JPY
    'Sweden',       # For SEK
    'Denmark',      # For DKK
    'New Zealand',  # For NZD
    'Norway',       # For NOK
    'Poland',       # For PLN
    'Peru',         # For PEN
    'Colombia',     # For COP
    'Paraguay',     # For PYG
    'Panama',       # For PAB
    'Hungary',      # For HUF
    'Rwanda',       # For RWF
    'Bulgaria',     # For BGN
    'South Africa', # For ZAR
    'Singapore',    # For SGD
    'Mauritius',    # For MUR
    'Chile'         # For CLP
]

# Apply the conditions and choices to create the country column
epmc_projects['country'] = np.select(conditions, choices, default='Unknown')

# Create mapping of funder names to country codes
funder_country_mapping = {
    'Cancer Research UK': 'GB',
    'Biotechnology and Biological Sciences Research Council': 'GB',
    'Wellcome Trust': 'GB',
    'ZonMw': 'NL',
    'Medical Research Council': 'GB',
    'Austrian Science Fund FWF': 'AT',
    'Versus Arthritis': 'GB',
    'Swiss National Science Foundation': 'CH',
    'British Heart Foundation': 'GB',
    'World Health Organization': 'CH',  # Based in Switzerland
    'Blood Cancer UK': 'GB',
    'Chief Scientist Office': 'GB',  # UK (Scotland)
    'National Medical Research Council Singapore': 'SG',
    'UK Research and Innovation': 'GB',
    'Breast Cancer Now': 'GB'
}

# Apply the override for rows where country is "Unknown"
for funder_name, country_code in funder_country_mapping.items():
    mask = (epmc_projects['country'] == 'Unknown') & (epmc_projects['funder_name'] == funder_name)
    epmc_projects.loc[mask, 'country'] = country_code

# Check how many rows were updated
print("Rows updated by funder:")
for funder_name in funder_country_mapping.keys():
    count = len(epmc_projects[(epmc_projects['funder_name'] == funder_name) &
                             (epmc_projects['country'] == funder_country_mapping[funder_name])])
    print(f"{funder_name}: {count} rows")

"""# Loading UKRI


"""

projects = '/content/drive/MyDrive/Bulk grant data/UKRI Gateway/UKRI_projects.csv'
df_projects = pd.read_csv(projects)
funds = '/content/drive/MyDrive/Bulk grant data/UKRI Gateway/ukri_funds_complete.csv'
df_funds = pd.read_csv(funds)
persons = '/content/drive/MyDrive/Bulk grant data/UKRI Gateway/ukri_persons_complete.csv'
df_persons = pd.read_csv(persons)
orgs = '/content/drive/MyDrive/Bulk grant data/UKRI Gateway/UKRI_orgs.csv'
df_orgs = pd.read_csv(orgs)

projects_funds = df_projects.merge(
    df_funds,
    on='project_id',
    how='left'
)

projects_funds_orgs = projects_funds.merge(
    df_orgs,
    left_on='lead_org_id',
    right_on='org_id',
    how='left'
)

projects_funds_orgs['lead_org_id'] = 'https://gtr.ukri.org/organisation/' + projects_funds_orgs['lead_org_id'].astype(str)

# Add country column based on region
projects_funds_orgs['country'] = projects_funds_orgs['region'].apply(
    lambda x: 'Unknown' if x == 'Unknown' else 'GB'
)

ukri_projects = projects_funds_orgs

# Create list of columns to unpivot
value_columns = [
    'coi_person_ids',
    'fellow_person_ids',
    'pi_person_ids',
    'student_person_ids',
    'super_person_ids'
]

# First get the base dataset with project info
projects_funds_orgs_melted = pd.melt(
    projects_funds_orgs,
    id_vars=['project_id'],
    value_vars=value_columns,
    var_name='role_type',
    value_name='person_id'
)

# Clean it up
projects_funds_orgs_melted = projects_funds_orgs_melted.dropna(subset=['person_id'])
projects_funds_orgs_melted = (
    projects_funds_orgs_melted.assign(
        person_id=projects_funds_orgs_melted['person_id'].str.split(';')
    )
    .explode('person_id'))
projects_funds_orgs_melted['person_id'] = projects_funds_orgs_melted['person_id'].str.strip()
projects_funds_orgs_melted = projects_funds_orgs_melted.drop_duplicates(
    subset=['project_id', 'person_id'])



# Step 1: First merge - combining with persons data
persons_merge = projects_funds_orgs_melted.merge(
    df_persons[['person_id', 'full_name', 'organisation_id']],
    on='person_id',
    how='left'
).merge(
    df_orgs[['org_id', 'name', 'region']],
    left_on='organisation_id',
    right_on='org_id',
    how='left'
)

# Step 2: Print info to verify the first merge worked
print("Shape after first merge:", persons_merge.shape)
print("Sample of merged data:")
print(persons_merge.head())

# Step 3: Second merge - adding project data
# Let's explicitly select both columns we're using from df_projects
projects_subset = df_projects[['project_id', 'grant_id']]

# Step 4: Final merge
final_merge = persons_merge.merge(
    projects_subset,
    on='project_id',
    how='left'
)

final_merge['person_id'] = 'https://gtr.ukri.org/person/' + final_merge['person_id']
ukri_awardees = final_merge

ukri_projects['country'].value_counts()

ukri_projects.columns

"""# Loading NIHR"""

nihr = '/content/drive/MyDrive/Bulk grant data/NIHR/NIHR.csv'
nihr_projects = pd.read_csv(nihr)

nihr_projects['funding_and_awards_link']

nihr_projects.columns

import pandas as pd
import numpy as np

# Define UK coordinate boundaries
# UK roughly spans:
# Latitude: 49.5째 to 61째 North
# Longitude: -8.5째 to 2째 East (negative values are West)
UK_LAT_MIN = 49.5
UK_LAT_MAX = 61.0
UK_LON_MIN = -8.5
UK_LON_MAX = 2.0

# Create country_final column
def assign_country(row):
    # Check if coordinates are available
    if pd.notna(row['latitude']) and pd.notna(row['longitude']):
        # Check if coordinates fall within UK bounds
        if (UK_LAT_MIN <= row['latitude'] <= UK_LAT_MAX and
            UK_LON_MIN <= row['longitude'] <= UK_LON_MAX):
            return 'GB'
        else:
            return row['institutioncountry']
    else:
        # If coordinates are missing, use institutioncountry
        return row['institutioncountry']

# Apply the function to create the new column
nihr_projects['country_final'] = nihr_projects.apply(assign_country, axis=1)

# Verify the results
print("Country assignment summary:")
print(nihr_projects['country_final'].value_counts())

# Check how many were assigned GB based on coordinates vs institutioncountry
gb_from_coords = ((nihr_projects['latitude'].between(UK_LAT_MIN, UK_LAT_MAX)) &
                  (nihr_projects['longitude'].between(UK_LON_MIN, UK_LON_MAX)) &
                  (nihr_projects['country_final'] == 'GB')).sum()

gb_from_institution = (nihr_projects['country_final'] == 'GB').sum() - gb_from_coords

print(f"\nGB assignments:")
print(f"From coordinates: {gb_from_coords}")
print(f"From institutioncountry: {gb_from_institution}")

# Create an explicit copy of the DataFrame
nihr_awardee_table = nihr_projects[[
    'project_id',
    "award_holder_name",
    "orcid",
    "involvement_type",
    'contracted_organisation',
    'organisation_type'
]].copy()

# Clean up the awardee names
nihr_awardee_table["award_holder_name"] = nihr_awardee_table["award_holder_name"].str.replace(
    r'\b(Professor|Dr|Associate|Mr|Ms|Miss|Mrs)\b', '', regex=True
).str.strip()


# Ensure the relevant columns are strings and handle NaN
for col in ["award_holder_name", "orcid", "involvement_type", "contracted_organisation", "organisation_type"]:
    nihr_awardee_table[col] = nihr_awardee_table[col].fillna("").astype(str)

def create_row_df(row):
    try:
        # Split all fields
        holders = row["award_holder_name"].split("/") if "/" in row["award_holder_name"] else [row["award_holder_name"]]
        ids = row["orcid"].split("/") if "/" in row["orcid"] else [row["orcid"]]
        roles = row["involvement_type"].split("/") if "/" in row["involvement_type"] else [row["involvement_type"]]
        orgs = row["contracted_organisation"].split("/") if "/" in row["contracted_organisation"] else [row["contracted_organisation"]]
        org_types = row["organisation_type"].split("/") if "/" in row["organisation_type"] else [row["organisation_type"]]

        # Get the number of award holders as our base length
        num_holders = len(holders)

        # If we have a single organization/org type, repeat it for all holders
        if len(orgs) == 1:
            orgs = orgs * num_holders
        if len(org_types) == 1:
            org_types = org_types * num_holders

        # Pad roles and ids if needed
        if len(roles) < num_holders:
            roles = roles + [''] * (num_holders - len(roles))
        if len(ids) < num_holders:
            ids = ids + [''] * (num_holders - len(ids))

        # Make sure all lists are exactly the same length as holders
        orgs = orgs[:num_holders]
        org_types = org_types[:num_holders]
        roles = roles[:num_holders]
        ids = ids[:num_holders]

        # Create lists for Project_ID
        project_ids = [row["project_id"]] * num_holders

        # Final length check with detailed debugging
        lengths = [len(project_ids), len(holders), len(ids), len(roles), len(orgs), len(org_types)]
        if len(set(lengths)) != 1:
            print(f"\nLength mismatch for Project ID {row['project_id']}:")
            print(f"project_ids ({len(project_ids)}): {project_ids}")
            print(f"holders ({len(holders)}): {holders}")
            print(f"ids ({len(ids)}): {ids}")
            print(f"roles ({len(roles)}): {roles}")
            print(f"orgs ({len(orgs)}): {orgs}")
            print(f"org_types ({len(org_types)}): {org_types}")
            print(f"All lengths: {lengths}")
            # Skip this row to continue processing
            return pd.DataFrame()

        # Create the DataFrame
        df_data = {
            "Project_ID": project_ids,
            "Award Holder": holders,
            "orcid": ids,
            "Role": roles,
            "Organisation": orgs,
            "Organisation Type": org_types
        }

        # Double-check before creating DataFrame
        for key, value in df_data.items():
            if len(value) != num_holders:
                print(f"\nFinal check failed for Project ID {row['project_id']}:")
                print(f"Column '{key}' has length {len(value)} but expected {num_holders}")
                print(f"Value: {value}")
                return pd.DataFrame()

        return pd.DataFrame(df_data)

    except Exception as e:
        print(f"\nUnexpected error processing Project ID {row['project_id']}:")
        print(f"Error: {str(e)}")
        print(f"Row data: {dict(row)}")
        # Return empty DataFrame to continue processing
        return pd.DataFrame()

# Let's first inspect a few rows before transformation
print("Sample of original data:")
print(nihr_awardee_table.head())

# Check for rows with "Unknown" organization
unknown_org_rows = nihr_awardee_table[nihr_awardee_table['contracted_organisation'].str.contains('Unknown|Not Known', case=False, na=False)]
print(f"\nRows with Unknown/Not Known organizations: {len(unknown_org_rows)}")
if len(unknown_org_rows) > 0:
    print("Sample:")
    print(unknown_org_rows[['project_id', 'contracted_organisation', 'organisation_type']].head())
    print(f"Organization types for these rows: {unknown_org_rows['organisation_type'].unique()}")

# Process each row individually to catch problematic ones
print(f"\nProcessing {len(nihr_awardee_table)} rows...")
long_awardee_rows = []
for idx, row in nihr_awardee_table.iterrows():
    if idx % 1000 == 0:
        print(f"Processing row {idx}...")
    result_df = create_row_df(row)
    if not result_df.empty:
        long_awardee_rows.append(result_df)

# Concatenate all the individual DataFrames
if long_awardee_rows:
    long_awardee_table = pd.concat(long_awardee_rows, ignore_index=True)

    # Clean up whitespace in all relevant columns
    columns_to_strip = ["Award Holder", "Role", "Organisation", "Organisation Type"]
    for col in columns_to_strip:
        long_awardee_table[col] = long_awardee_table[col].str.strip()

    # Remove rows where Award Holder is empty
    long_awardee_table = long_awardee_table[long_awardee_table["Award Holder"] != ""]

    # Add ORCID prefix only to non-empty ORCID values, excluding "Not Provided"
    long_awardee_table['orcid'] = long_awardee_table['orcid'].apply(
        lambda x: 'orcid.org/' + x if x and x.strip() and x.strip().lower() != 'not provided' else ''
    )

    print(f"\nSuccessfully processed data. Final table has {len(long_awardee_table)} rows.")
else:
    print("\nNo valid rows were processed!")
    long_awardee_table = pd.DataFrame()


nihr_awardees = long_awardee_table

"""# Loading European Commission"""

ec_projects = pd.read_csv('/content/drive/MyDrive/Bulk grant data/European Commission/EC full projects.csv')
ec_awardees = pd.read_csv('/content/drive/MyDrive/Bulk grant data/European Commission/EC full organizations.csv')
ec_awardees['award_holder_name'] = ec_awardees['firstName'].fillna('') + ' ' + ec_awardees['lastName'].fillna('')

# Filter ec_awardees for coordinators only
coordinators = ec_awardees[ec_awardees['role'] == 'coordinator']

# Merge with ec_projects
ec_projects = ec_projects.merge(
    coordinators[['projectID', 'name', 'country', 'organizationURL']],
    left_on='id',
    right_on='projectID',
    how='left'
).drop_duplicates(subset='id')

"""# Bringing it all together

## Projects
"""

# Harmonise columns across datasets
# EPMC
epmc_common = epmc_projects.rename(columns={
    'grant_id': 'grant_id',
    'grant_title': 'title',
    'scientific_abstract': 'abstract',
 #   'lay_abstract': 'lay_abstract',
    'grant_type': 'grant_category',
 #   'grant_stream': 'grant_stream',
    'funder_name': 'funder',
    'institution_name': 'lead_org_name',
    'institution_ror_id': 'lead_org_id',
   'country': 'country',
    'start_date': 'start_date',
    'end_date': 'end_date',
    'grant_amount': 'grant_amount',
    'grant_currency': 'grant_currency'
}).assign(
    source='Europe PMC')[['grant_id', 'source', 'title', 'abstract', 'grant_category', 'funder', 'lead_org_name','lead_org_id', 'country', 'start_date', 'end_date', 'grant_amount', 'grant_currency']]

# NIHR

nihr_common = nihr_projects.rename(columns={
    'project_id': 'grant_id',
   # 'funding_and_awards_link': 'grant_url',
    'project_title': 'title',
    'scientific_abstract': 'abstract',
  #  'plain_english_abstract': 'lay_abstract',
    'programme_type': 'grant_category',
 #   'programme': 'grant_stream',
    'funder': 'funder',
    'contracted_organisation': 'lead_org_name',
    'institutioncountry': 'country',
    'start_date': 'start_date',
    'end_date': 'end_date',
    'award_amount_from_dh': 'grant_amount'
}).assign(
    source='NIHR Funding and Awards',
    **{'grant_currency': 'GBP'})[['grant_id', 'source', 'title', 'abstract', 'grant_category', 'funder', 'lead_org_name', 'country', 'start_date', 'end_date', 'grant_amount', 'grant_currency']]

# UKRI
ukri_common = ukri_projects.rename(columns={
    'grant_id': 'grant_id',
    'title': 'title',
    'abstract': 'abstract',
    'grant_category': 'grant_category',
    'lead_funder': 'funder',
    'name': 'lead_org_name',
    'lead_org_id': 'lead_org_id',
    'country': 'country',
    'start_date': 'start_date',
    'end_date': 'end_date',
    'amount': 'grant_amount',
    'currency': 'grant_currency'
}).assign(
    source = 'UKRI Gateway to Research')[['grant_id', 'source', 'title', 'abstract', 'grant_category', 'funder', 'lead_org_name','lead_org_id','country', 'start_date', 'end_date', 'grant_amount', 'grant_currency']]


# Rename existing columns and create new ones in one step
ec_common = ec_projects.rename(columns={
    'id': 'grant_id',
    'title': 'title',
    'objective': 'abstract',
    'frameworkProgramme': 'grant_category',
    'fundingScheme': 'grant_stream',
    'name': 'lead_org_name',
    'country': 'country',
    'organizationURL': 'lead_org_id',
    'startDate': 'start_date',
    'endDate': 'end_date',
    'totalCost': 'grant_amount'
}).assign(
    funder='European Commission',
    **{'grant_currency': 'EUR'},
    **{'source': 'European Commission'}# Add this if the column doesn't exist
)[['grant_id', 'source', 'title', 'abstract', 'grant_category', 'funder', 'lead_org_name','lead_org_id', 'country',  'start_date', 'end_date', 'grant_amount', 'grant_currency']]

# First, combine UKRI and NIHR datasets
priority_df = pd.concat([ukri_common, nihr_common], ignore_index=True)

# Get list of IDs from UKRI and NIHR
existing_ids = priority_df['grant_id'].unique()

# Filter EPMC to only include rows where the ID isn't in UKRI or NIHR
epmc_unique = epmc_common[~epmc_common['grant_id'].isin(existing_ids)]
epmc_unique = epmc_unique.loc[:, ~epmc_unique.columns.duplicated()]

# Now combine all datasets
final_combined_df = pd.concat([priority_df, epmc_unique, ec_common], ignore_index=True)
final_combined_df = final_combined_df.dropna(subset=['grant_id'])

# Analyze column quality (% empty/missing values)
def analyze_column_quality(df):
    """Analyze the percentage of missing values in each column"""

    total_rows = len(df)
    quality_stats = []

    for column in df.columns:
        # Count different types of missing values
        null_count = df[column].isnull().sum()
        empty_string_count = (df[column] == '').sum() if df[column].dtype == 'object' else 0
        total_missing = null_count + empty_string_count

        # Calculate percentages
        missing_pct = (total_missing / total_rows) * 100
        complete_pct = 100 - missing_pct

        quality_stats.append({
            'column': column,
            'total_rows': total_rows,
            'missing_count': total_missing,
            'missing_percentage': round(missing_pct, 2),
            'complete_percentage': round(complete_pct, 2),
            'data_type': str(df[column].dtype)
        })

    # Create summary DataFrame
    quality_df = pd.DataFrame(quality_stats)

    # Sort by missing percentage (highest first)
    quality_df = quality_df.sort_values('missing_percentage', ascending=False)

    return quality_df

# Run the analysis
column_quality = analyze_column_quality(final_combined_df)
print("Column Quality Analysis:")
print("=" * 80)
print(column_quality.to_string(index=False))

# Summary statistics
print("\n" + "=" * 80)
print("SUMMARY:")
print(f"Total rows in dataset: {len(final_combined_df):,}")
print(f"Total columns: {len(final_combined_df.columns)}")
print(f"Columns with >50% missing: {len(column_quality[column_quality['missing_percentage'] > 50])}")
print(f"Columns with >25% missing: {len(column_quality[column_quality['missing_percentage'] > 25])}")
print(f"Columns with complete data: {len(column_quality[column_quality['missing_percentage'] == 0])}")

# If you want to compare quality across data sources
# Add source labels before concatenating (in your original code):

# When creating the dataframes, add a source column:
# priority_df['source'] = 'UKRI_NIHR'
# epmc_unique['source'] = 'EPMC'
# ec_common['source'] = 'EC'

# Then analyze by source:
def analyze_by_source(df):
    """Analyze column quality grouped by data source"""
    if 'source' in df.columns:
        quality_by_source = df.groupby('source').apply(
            lambda x: pd.Series({
                col: f"{(x[col].isnull().sum() / len(x)) * 100:.1f}%"
                for col in x.columns if col != 'source'
            })
        )
        return quality_by_source
    else:
        print("No 'source' column found. Add source labels to track data origin.")
        return None

analyze_by_source(final_combined_df)

"""## Awardees"""

# Function to standardize data format
def standardize_data(df, source):
    if source == 'NIHR Funding and Awards':
        standardized = pd.DataFrame({
            'researcher_name': df['Award Holder'],
            'researcher_id': df['orcid'],
            'grant_id': df['Project_ID'],
            'source': source,
            'role': df['Role'],
            'organisation': df['Organisation'],
    #        'country': df['country']

        })

    elif source == 'UKRI Gateway to Research':
        standardized = pd.DataFrame({
            'researcher_name': df['full_name'],
            'researcher_id': df['person_id'],
            'grant_id': df['grant_id'],
            'source': source,
            'role': df['role_type'],
            'organisation': df['name'],
     #       'country': df['country']
        })

    elif source == 'Europe PMC':
        standardized = pd.DataFrame({
            'researcher_name': df['award_holder_name'],
            'researcher_id': df['orcid'],
            'grant_id': df['grant_id'],
            'source': source,
            'role': "EPMC Awardee",
            'organisation': df['institution_name'],
      #      'country': df['country']
        })

    elif source == 'European Commission':
        standardized = pd.DataFrame({
            'researcher_name': df['award_holder_name'],
         #   'researcher_id': df['orcid'],
            'grant_id': df['projectID'],
            'source': source,
            'role': "role",
            'organisation': df['name'],
       #     'country': df['country']
        })

    return standardized

# Standardize each dataset
nihr_standardized = standardize_data(nihr_awardees, 'NIHR Funding and Awards')
ukri_standardized = standardize_data(ukri_awardees, 'UKRI Gateway to Research')
epmc_standardized = standardize_data(epmc_awardees, 'Europe PMC')
ec_standardized = standardize_data(ec_awardees, 'European Commission')

# First, combine only UKRI, NIHR, and EPMC for deduplication
# (excluding EC because most researcher names are empty)
combined_for_dedup = pd.concat([
    ukri_standardized,    # Highest priority
    nihr_standardized,    # Second priority
    epmc_standardized     # Third priority
], ignore_index=True)

# Print statistics before deduplication
print("\nDataset sizes before deduplication:")
print(f"NIHR entries: {len(nihr_standardized)}")
print(f"UKRI entries: {len(ukri_standardized)}")
print(f"EPMC entries: {len(epmc_standardized)}")
print(f"EC entries: {len(ec_standardized)} (excluded from researcher name deduplication)")
print(f"Combined dataset size (for deduplication): {len(combined_for_dedup)}")

# Remove duplicates based on researcher_name and grant_id
# Keep first occurrence (highest priority source)
# Only applies to UKRI, NIHR, and EPMC data
deduplicated_subset = combined_for_dedup.drop_duplicates(
    subset=['researcher_name', 'grant_id'],
    keep='first'
)

# Now add EC data back to the deduplicated dataset
combined_authors_dedup = pd.concat([
    deduplicated_subset,
    ec_standardized
], ignore_index=True)

# Print statistics after deduplication
print("\nDataset sizes after deduplication:")
print(f"Deduplicated subset (UKRI/NIHR/EPMC): {len(deduplicated_subset)}")
print(f"Duplicates removed from UKRI/NIHR/EPMC: {len(combined_for_dedup) - len(deduplicated_subset)}")
print(f"Total combined dataset size (including EC): {len(combined_authors_dedup)}")

# Check distribution by source after deduplication
print("\nRecords by source after deduplication:")
source_counts = combined_authors_dedup['source'].value_counts()
for source, count in source_counts.items():
    print(f"{source}: {count}")

# Optional: Show examples of duplicates that were removed from UKRI/NIHR/EPMC
print("\nChecking for any remaining duplicates in UKRI/NIHR/EPMC subset:")
duplicate_check = deduplicated_subset.duplicated(subset=['researcher_name', 'grant_id'])
remaining_duplicates = duplicate_check.sum()
print(f"Remaining duplicates in deduplicated subset: {remaining_duplicates}")

# Optional: Save the deduplicated dataset
# combined_authors_dedup.to_csv('combined_authors_deduplicated.csv', index=False)

# Then analyze by source:
def analyze_by_source(df):
    """Analyze column quality grouped by data source"""
    if 'source' in df.columns:
        quality_by_source = df.groupby('source').apply(
            lambda x: pd.Series({
                col: f"{(x[col].isnull().sum() / len(x)) * 100:.1f}%"
                for col in x.columns if col != 'source'
            })
        )
        return quality_by_source
    else:
        print("No 'source' column found. Add source labels to track data origin.")
        return None

analyze_by_source(combined_authors_dedup)